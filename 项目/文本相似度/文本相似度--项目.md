项目原理：

​		基于词频：向同级词频，然后构建词频特征向量，利用特征向量夹角余弦表示文本相似度。

​		基于语义：基于语料统计的方法中的word-Net，How-Net词典提取相关语义特征。

流程：

对文本进行分词，去掉其中的停用词，然后统计每个词出现的次数，构建词频向量，通过计算向量夹角来反应两个文本的相似度。

# 遇到的问题

## 转码问题

cppjieba在windows中不能直接使用，在Linux中可以直接使用，因为在Linux中默认的编码是UTF-8，但是windows中的默认的编码是GBK，cppjieba是针对UTF-8来分词。要将GBK（2字符）编码转换为UTF-8（万国码，没有固定长度），才可以。

UTF-8不能喝GBK直接进行转码，因为他们没有对应的映射关系，要先将GBK转换为UTF-16，然后将UTF-16转换为UTF-8，用编译器中自带的编码转换工具。

```
#include <windows.h>
string GBKtoUTF8(string str)
{
	//获取buf大小
    int len = MultiByteToWideChar(CP_ACP, 0, str.c_str(), -1, NULL,0);
    //最后一个参数为buf的大小，如果设置为0，那么这个函数的返回值就会这个字符串所需要的buf的大小
    //定义一个宽字符buf
    wchar_t* wstr = new wchar_t[len];
    //将GBK转换为UTF16
    MultiByteToWideChar(CP_ACP, 0, str.c_str(), -1, wstr, len);
    lenlen = WideCharToMultiByte(CP_UTF8, 0, wstr, -1, NULL, 0, NULL, NULL);
    char* utf8 = new char[len];
    WideCharToMultiByte(CP_UTF8, 0, wstr, -1, utf8, len, NULL, NULL);
    
    string ret(utf8);
    if(wstr)
    {
    	delete[] wstr;
    	wstr = nullptr;
    }
    if(utf8)
    {
    	delete[] utf8;
    	utf8 = nullptr;
    }
    return ret;
}
string UTF8toGBK(string str)
{
	int len = MultiByteToWideChar(CP_UTF8, 0, str.c_str(), -1, NULL,0);
	wchar_t* wstr = new wchar_t[len];
	MultiByteToWideChar(CP_UTF8, 0, str.c_str(), -1, wstr, len);
	
	len = WideCharToMultiByte(CP_GBK, 0, wstr, -1, NULL, 0, NULL, NULL);
	char* GBK = new char[len];
    WideCharToMultiByte(CP_GBK, 0, wstr, -1, utf8, len, NULL, NULL);
	
	string ret(GBK);
    if(wstr)
    {
    	delete[] wstr;
    	wstr = nullptr;
    }
    if(GBK)
    {
    	delete[] GBK;
    	GBK = nullptr;
    }
    return ret;
}
```

# 很多分词软件为什么要用jieba？

本篇文章测试的哈工大LTP、中科院计算所NLPIR、清华大学THULAC和jieba、FoolNLTK、HanLP这六大中文分词工具是由  [水...琥珀](https://blog.csdn.net/shuihupo) 完成的。相关测试的文章之前也看到过一些，但本篇阐述的可以说是比较详细的了。这里就分享一下给各位朋友！

**安装调用**

jieba“结巴”中文分词：做最好的 Python 中文分词组件

THULAC清华大学：一个高效的中文词法分析工具包

FoolNLTK可能不是最快的开源中文分词，但很可能是最准的开源中文分词

教程：FoolNLTK 及 HanLP使用

HanLP最高分词速度2,000万字/秒

**中科院 Ictclas 分词系统 - NLPIR汉语分词系统

哈工大 LTP

LTP安装教程[python 哈工大NTP分词 安装pyltp 及配置模型（新）]

如下是测试代码及结果

 

![img](https://img-blog.csdnimg.cn/20190225100718583.JPG)

下面测试的文本上是极易分词错误的文本，分词的效果在很大程度上就可以提现分词器的分词情况。接下来验证一下，分词器的宣传语是否得当吧。

![img](https://img-blog.csdnimg.cn/20190225102258435.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

 

**jieba 中文分词**

 

![img](https://img-blog.csdnimg.cn/20190225102310650.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

**thulac 中文分词**

 

![img](https://img-blog.csdnimg.cn/20190225102322290.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

**fool 中文分词**

![img](https://img-blog.csdnimg.cn/20190225102333992.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

 

**HanLP 中文分词**

 

![img](https://img-blog.csdnimg.cn/20190225102345924.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

**中科院分词 nlpir**

 

![img](https://img-blog.csdnimg.cn/20190225102356686.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

**哈工大ltp 分词**

 

![img](https://img-blog.csdnimg.cn/20190225102408579.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

以上可以看出分词的时间，为了方便比较进行如下操作：

 

**分词效果对比**

![img](https://img-blog.csdnimg.cn/20190225102423546.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190225102436630.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190225102446508.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

 

**结果为：**

![img](https://img-blog.csdnimg.cn/20190225102503627.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190225102512617.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190225102526515.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190225102536481.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

**总结：**

1.时间上（不包括加载包的时间），对于相同的文本测试两次，四个分词器时间分别为：

jieba:    0.01699233055114746 1.8318662643432617

thulac ： 10.118737936019897 8.155954599380493

fool： 2.227612018585205 2.892209053039551

HanLP： 3.6987085342407227 1.443108320236206

中科院nlpir：0.002994060516357422

哈工大ltp_ ：0.09294390678405762

**可以看出平均耗时最短的是中科院nlpir分词，最长的是thulac，时间的差异还是比较大的。**

2.分词准确率上，通过分词效果操作可以看出

第一句：结婚的和尚未结婚的确实在干扰分词啊

四个分词器都表现良好，唯一不同的是fool将“干扰分词”合为一个词

第二句：汽水不如果汁好喝，重点在“不如果”，“”不如“” 和“”如果“” 在中文中都可以成词，但是在这个句子里是不如 与果汁 正确分词

jieba  thulac fool HanLP

jieba、 fool 、HanLP正确  thulac错误

第三句： 小白痴痴地在门前等小黑回来，体现在人名的合理分词上

正确是：

小白/ 痴痴地/ 在/ 门前/ 等/ 小黑/ 回来

jieba、 fool 、HanLP正确，thulac在两处分词错误： 小白_np 痴痴_a 地_u 在_p 门前_s 等_u 小_a 黑回_n 来_f

第四句：是有关司法领域文本分词

发现HanLP的分词粒度比较大，fool分词粒度较小，导致fool分词在上有较大的误差。在人名识别上没有太大的差异，在组织机构名上分词，分词的颗粒度有一些差异，Hanlp在机构名的分词上略胜一筹。

**六种分词器使用建议：**

对命名实体识别要求较高的可以选择HanLP，根据说明其训练的语料比较多，载入了很多实体库，通过测试在实体边界的识别上有一定的优势。

中科院的分词，是学术界比较权威的，对比来看哈工大的分词器也具有比较高的优势。同时这两款分词器的安装虽然不难，但比较jieba的安装显得繁琐一点，代码迁移性会相对弱一点。哈工大分词器pyltp安装配置模型教程

结巴因为其安装简单，有三种模式和其他功能，支持语言广泛，流行度比较高，且在操作文件上有比较好的方法好用python -m jieba news.txt > cut_result.txt

对于分词器的其他功能就可以在文章开头的链接查看，比如说哈工大的pyltp在命名实体识别方面，可以输出标注的词向量，是非常方便基础研究的命名实体的标注工作。

![img](https://img-blog.csdnimg.cn/20190225102548539.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

 

**精简文本 效果对比**

 

![img](https://img-blog.csdnimg.cn/2019022510260559.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/2019022510261521.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkbmIzNGc=,size_16,color_FFFFFF,t_70)

 

https://blog.csdn.net/shuihupo/article/details/81540433



为什么分词的方法为HMM？

jieba中的分词方法有MP,HMM,Mix，在选择的时候记得最深的就是他来到网易杭研大厦这句话

MP方法：他/来到/了/网易/杭/研/大厦

HMM方法：他来/到/了/网易/杭/研大厦

Mix方法：他/来到/了/网易/杭研/大厦

可以看出效果最好的是Mix，也就是融合MP和HMM的切词算法。即可以准确切出词典已有的词，又可以切出像"杭研"这样的未登录词。

怎么统计词频？

在统计词频这块使用的是unordered_map容器，他的底层结构是哈希表，发招速度要比map快，统计词频的方法就是产看当前词组是否已经在容器中，如果在就对他相应的位置进行++，如果没有说明这个词组是第一次出现，将他对应的位置置1。

为什么要用词频向量？怎么构建？

排序的时候sort不能直接对kv结构的数据进行排序，只能对线性的数据进行排序，所以要将map中的数据全部保存到vector中，然后定义一个比较函数，才可以进行排序。



多个文件比较的时候怎么办？

如果这两个文件很大怎么办？



# 项目介绍

这个项目是有一次我去找老师问问题的时候听到他们办公室在讨论今年的论文查重的话题，然后我就开始和老师聊这个论文查重，论文查重主要就是看两篇文章的相似度。

这个项目的原理就是先将两个文档通过jiebacpp进行分词，使用jiebacpp中的停用词表将里面的停用词，的，了之类的去掉，然后统计这些词的出现的频率，因为一个文章中如果某些词出现的次数比较多，大概就可以确定这个文章主要讲述了什么，将已经处理完的词根据每个词的词频进行

隐马尔可夫（HMM）